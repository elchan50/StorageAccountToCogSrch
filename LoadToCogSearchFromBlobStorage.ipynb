{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  Files from Storage Account into CogSearch \n",
    "### The files from Storage Account are loaded into CogSearch by following below steps\n",
    "- Establish a connection with Storage Account using the Python SDK.\n",
    "- Retrieve the required files from Storage Account container using the file stream download method.\n",
    "- Use Azure AI Document Intelligence to parse the PDF files from Storage Account.\n",
    "- Divide the PDF into smaller, manageable page chunks for easier processing.\n",
    "- Index the parsed chunks into Azure Cognitive Search.\n",
    "- Repeat the process for all the required files.\n",
    "\n",
    "#### Using the Azure Storage Pyhon SDK  to fetch the file stream and use PYPDF and Document Intelligence to chunk the page in memory and create a vector index\n",
    "- Azure Python SDK https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob\n",
    "- Refer to https://github.com/MSUSAzureAccelerators/Azure-Cognitive-Search-Azure-OpenAI-Accelerator/blob/main/04-Complex-Docs.ipynb for loading large documents using PYPDF and Document Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required Packages and setup the auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import shutil\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter,PdfReader \n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import html\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from azure.storage.blob.aio import BlobClient\n",
    "from azure.storage.blob import ContainerClient\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "from azure.storage.blob import BlobClient\n",
    "import io  \n",
    "\n",
    "# Configure environment variables  \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header for cog search\n",
    "headers = {'Content-Type': 'application/json','api-key': os.getenv('AZURE_SEARCH_KEY')}\n",
    "\n",
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to Parse the PDF using the PYPDF or FormRecognizer ,  stream of file content from Storage Account is source to parse the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(filename, form_recognizer=False, formrecognizer_endpoint=None, formrecognizerkey=None, model=\"prebuilt-document\", from_url=False, verbose=False):\n",
    "    \"\"\"Parses PDFs using PyPDF or Azure Document Intelligence SDK (former Azure Form Recognizer)\"\"\"\n",
    "    \n",
    "    page_map=[]\n",
    "    #setup a blob client with authentication\n",
    "    blob_client = BlobClient.from_connection_string(conn_str=os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\"), \n",
    "                                                    container_name=os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\"), blob_name=filename)\n",
    "    #get stream \n",
    "    stream = blob_client.download_blob()\n",
    "    file_content_stream = stream.readall()\n",
    "    offset = 0\n",
    "    \n",
    "    if not form_recognizer:\n",
    "        if verbose: print(f\"Extracting text using PyPDF\")\n",
    "        reader = PdfReader(file_content_stream)\n",
    "        pages = reader.pages\n",
    "        for page_num, p in enumerate(pages):\n",
    "            page_text = p.extract_text()\n",
    "            page_map.append((page_num, offset, page_text))\n",
    "            offset += len(page_text)\n",
    "    else:\n",
    "        if verbose: print(f\"Extracting text using Azure Document Intelligence\")\n",
    "        credential = AzureKeyCredential(os.getenv(\"FORM_RECOGNIZER_KEY\"))\n",
    "        form_recognizer_client = DocumentAnalysisClient(endpoint=os.getenv(\"FORM_RECOGNIZER_ENDPOINT\"), credential=credential)\n",
    "        \n",
    "        if not from_url:\n",
    "            #with open(file, \"rb\") as filename:\n",
    "            poller = form_recognizer_client.begin_analyze_document(model, document = file_content_stream)\n",
    "        #else:\n",
    "        #    poller = form_recognizer_client.begin_analyze_document_from_url(model, document_url = file)\n",
    "            \n",
    "        form_recognizer_results = poller.result()\n",
    "\n",
    "        for page_num, page in enumerate(form_recognizer_results.pages):\n",
    "            tables_on_page = [table for table in form_recognizer_results.tables if table.bounding_regions[0].page_number == page_num + 1]\n",
    "\n",
    "            # mark all positions of the table spans in the page\n",
    "            page_offset = page.spans[0].offset\n",
    "            page_length = page.spans[0].length\n",
    "            table_chars = [-1]*page_length\n",
    "            for table_id, table in enumerate(tables_on_page):\n",
    "                for span in table.spans:\n",
    "                    # replace all table spans with \"table_id\" in table_chars array\n",
    "                    for i in range(span.length):\n",
    "                        idx = span.offset - page_offset + i\n",
    "                        if idx >=0 and idx < page_length:\n",
    "                            table_chars[idx] = table_id\n",
    "\n",
    "            # build page text by replacing charcters in table spans with table html\n",
    "            page_text = \"\"\n",
    "            added_tables = set()\n",
    "            for idx, table_id in enumerate(table_chars):\n",
    "                if table_id == -1:\n",
    "                    page_text += form_recognizer_results.content[page_offset + idx]\n",
    "                elif not table_id in added_tables:\n",
    "                    page_text += table_to_html(tables_on_page[table_id])\n",
    "                    added_tables.add(table_id)\n",
    "\n",
    "            page_text += \" \"\n",
    "            page_map.append((page_num, offset, page_text))\n",
    "            offset += len(page_text)\n",
    "\n",
    "    return page_map    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tables in pdf convert to html\n",
    "def table_to_html(table):\n",
    "    table_html = \"<table>\"\n",
    "    rows = [sorted([cell for cell in table.cells if cell.row_index == i], key=lambda cell: cell.column_index) for i in range(table.row_count)]\n",
    "    for row_cells in rows:\n",
    "        table_html += \"<tr>\"\n",
    "        for cell in row_cells:\n",
    "            tag = \"th\" if (cell.kind == \"columnHeader\" or cell.kind == \"rowHeader\") else \"td\"\n",
    "            cell_spans = \"\"\n",
    "            if cell.column_span > 1: cell_spans += f\" colSpan={cell.column_span}\"\n",
    "            if cell.row_span > 1: cell_spans += f\" rowSpan={cell.row_span}\"\n",
    "            table_html += f\"<{tag}{cell_spans}>{html.escape(cell.content)}</{tag}>\"\n",
    "        table_html +=\"</tr>\"\n",
    "    table_html += \"</table>\"\n",
    "    return table_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_base64(text):\n",
    "    # Convert text to bytes using UTF-8 encoding\n",
    "    bytes_data = text.encode('utf-8')\n",
    "\n",
    "    # Perform Base64 encoding\n",
    "    base64_encoded = base64.b64encode(bytes_data)\n",
    "\n",
    "    # Convert the result back to a UTF-8 string representation\n",
    "    base64_text = base64_encoded.decode('utf-8')\n",
    "\n",
    "    return base64_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a list for holding the file details \n",
    "files_to_index = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the blobs in your container using the ContainerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = ContainerClient.from_connection_string(conn_str=os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\"), container_name=os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\"))\n",
    "\n",
    "blob_list = container.list_blobs()\n",
    "for blob in blob_list:\n",
    "    files_to_index.append({\"file_name\": blob.name,\"file_url\": (os.getenv(\"AZURE_STORAGE_BASE_URL\")+ os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\") + \"/\" + blob.name)})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the file content Stream for the blobs async and use the document intelligence to chunk pdf pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in files_to_index:\n",
    "    print(item)\n",
    "    item[\"page_map\"] = parse_pdf(filename=item[\"file_name\"], form_recognizer=True, model=\"prebuilt-document\",from_url=False, verbose=True)\n",
    "    #item[\"page_map\"] = parse_pdf(filename=item[\"file_name\"], form_recognizer=True, model=\"prebuilt-layout\",from_url=False, verbose=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_to_index[0][\"page_map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_index_name = \"idx_fabric_get\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Azure Search Vector-based Index\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.getenv('AZURE_SEARCH_KEY')}\n",
    "params = {'api-version': os.getenv('AZURE_SEARCH_API_VERSION')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from azure.storage.blob import BlobClient\n",
    "\n",
    "#blob = BlobClient.from_connection_string(conn_str=os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\"), container_name=os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\"), blob_name=\"004003-Contract-Client-2023Oct18-232541.pdf\")\n",
    "#stream = blob.download_blob()\n",
    "#data = stream.readall()\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the content of the box file chunks (each page of each file) in the dictionary page_map, let's create the Vector-based index in our Azure Search Engine where this content is going to land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "index_payload = {\n",
    "    \"name\": str_index_name,\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
    "        {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunk\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunkVector\",\"type\": \"Collection(Edm.Single)\",\"searchable\": \"true\",\"retrievable\": \"true\",\"dimensions\": 1536, \"vectorSearchProfile\": \"my-default-vector-profile\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"page_num\",\"type\": \"Edm.Int32\",\"searchable\": \"false\",\"retrievable\": \"true\"},\n",
    "        \n",
    "    ],\n",
    "     \"vectorSearch\": {\n",
    "        \"algorithms\": [\n",
    "            {\n",
    "                \"name\": \"my-hnsw-config-1\",\n",
    "                \"kind\": \"hnsw\",\n",
    "                \"hnswParameters\": {\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"profiles\": [\n",
    "            {\n",
    "                \"name\": \"my-default-vector-profile\",\n",
    "                \"algorithm\": \"my-hnsw-config-1\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.getenv('AZURE_SEARCH_ENDPOINT') + \"/indexes/\" + str_index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment in case of errors\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Document chunks and its vectors to the Vector-Based Index\n",
    "The following code will iterate over each chunk of each book and use the Azure Search Rest API upload method to insert each document with its corresponding vector (using OpenAI embedding model) to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for item in files_to_index:\n",
    "    print(\"Uploading chunks from\",item[\"file_name\"])\n",
    "    for page in tqdm(item['page_map']):\n",
    "        try:\n",
    "            page_num = page[0] + 1\n",
    "            content = page[2]\n",
    "            book_url = item[\"file_url\"]\n",
    "            idx_id = text_to_base64(item[\"file_name\"] + str(page_num))\n",
    "            title = f\"{item['file_name']}_page_{str(page_num)}\"  \n",
    "            upload_payload = {\n",
    "                \"value\": [\n",
    "                    {\n",
    "                        \"id\": idx_id,\n",
    "                        \"title\": title,\n",
    "                        \"chunk\": content,\n",
    "                        \"chunkVector\": embedder.embed_query(content if content!=\"\" else \"-------\"),\n",
    "                        \"name\": item[\"file_name\"],\n",
    "                        \"location\": item[\"file_url\"],\n",
    "                        \"page_num\": page_num,\n",
    "                        \"@search.action\": \"upload\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + str_index_name + \"/docs/index\",\n",
    "                                 data=json.dumps(upload_payload), headers=headers, params=params)\n",
    "            if r.status_code != 200:\n",
    "                print(r.status_code)\n",
    "                print(r.text)\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\",e)\n",
    "            print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
